# H∆∞·ªõng D·∫´n C·∫•u H√¨nh Provider Fallback

## üìã T·ªïng Quan

H·ªá th·ªëng `ai-mcp-gateway` gi·ªù ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p v·ªõi kh·∫£ nƒÉng t·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi gi·ªØa c√°c LLM providers d·ª±a tr√™n t√¨nh tr·∫°ng k·∫øt n·ªëi th·ª±c t·∫ø. Khi m·ªôt provider fail, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông fallback sang provider kh√°c c√≤n ho·∫°t ƒë·ªông.

## üîÑ C∆° Ch·∫ø Fallback

### Th·ª© T·ª± ∆Øu Ti√™n:
1. **Provider g·ªëc** (OpenAI ho·∫∑c Claude n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh)
2. **OpenRouter** (v·ªõi models ƒë∆∞·ª£c c·∫•u h√¨nh thay th·∫ø)
3. **OSS Local** (Ollama - n·∫øu ƒë∆∞·ª£c b·∫≠t)

### Khi N√†o Fallback X·∫£y Ra:
- Provider kh√¥ng c√≥ API key
- Provider kh√¥ng ph·∫£n h·ªìi (timeout)
- Provider tr·∫£ v·ªÅ l·ªói (rate limit, outage, etc.)
- Health check ƒë√°nh d·∫•u provider kh√¥ng kh·∫£ d·ª•ng

## ‚öôÔ∏è C·∫•u H√¨nh File .env

### 1. **Ch·ªâ D√πng OpenRouter** (Recommended cho b·∫Øt ƒë·∫ßu)

```bash
# Ch·ªâ c·∫ßn OpenRouter API key
OPENROUTER_API_KEY=your_key_here

# Kh√¥ng c·∫ßn OpenAI/Claude
# OPENAI_API_KEY=
# ANTHROPIC_API_KEY=

# Models fallback (free models)
OPENROUTER_FALLBACK_MODELS=x-ai/grok-beta,qwen/qwen-2.5-coder-32b-instruct,meta-llama/llama-3.1-8b-instruct:free

# Models thay th·∫ø (n·∫øu c√≥ request c·∫ßn OpenAI/Claude format)
OPENROUTER_REPLACE_OPENAI=openai/gpt-4o-mini
OPENROUTER_REPLACE_CLAUDE=anthropic/claude-3.5-sonnet
```

### 2. **OpenAI + OpenRouter Fallback**

```bash
# Primary provider
OPENAI_API_KEY=your_openai_key

# Fallback provider
OPENROUTER_API_KEY=your_openrouter_key

# Model thay th·∫ø khi OpenAI fail
OPENROUTER_REPLACE_OPENAI=openai/gpt-4o-mini

# Free models cho fallback
OPENROUTER_FALLBACK_MODELS=x-ai/grok-beta,qwen/qwen-2.5-coder-32b-instruct
```

### 3. **OpenRouter + Local Model**

```bash
# Primary
OPENROUTER_API_KEY=your_key

# Secondary: Local Ollama
OSS_MODEL_ENABLED=true
OSS_MODEL_ENDPOINT=http://localhost:11434
OSS_MODEL_NAME=llama3:8b

OPENROUTER_FALLBACK_MODELS=x-ai/grok-beta,qwen/qwen-2.5-coder-32b-instruct
```

### 4. **Full Stack** (T·∫•t c·∫£ providers)

```bash
# T·∫•t c·∫£ providers
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_claude_key
OPENROUTER_API_KEY=your_openrouter_key

# Local backup
OSS_MODEL_ENABLED=true
OSS_MODEL_ENDPOINT=http://localhost:11434
OSS_MODEL_NAME=llama3:8b

# C·∫•u h√¨nh thay th·∫ø
OPENROUTER_REPLACE_OPENAI=openai/gpt-4o-mini
OPENROUTER_REPLACE_CLAUDE=anthropic/claude-3.5-sonnet
OPENROUTER_FALLBACK_MODELS=x-ai/grok-beta,qwen/qwen-2.5-coder-32b-instruct,meta-llama/llama-3.1-8b-instruct:free
```

## üöÄ Ch·∫°y Server

```bash
# Build
npm run build

# Start API server
npm run start:api

# Ho·∫∑c v·ªõi PowerShell
$env:MODE="api"; node dist/index.js
```

## üìä Log Khi Kh·ªüi ƒê·ªông

Server s·∫Ω log tr·∫°ng th√°i c√°c providers:

```
Checking LLM provider connectivity...
openai: ‚úÖ Available
anthropic: ‚ùå Unavailable
openrouter: ‚úÖ Available
oss-local: ‚ùå Unavailable
```

## üîç V√≠ D·ª• Fallback Flow

### Scenario 1: OpenAI Fail ‚Üí OpenRouter
```
1. Request ƒë·∫øn v·ªõi model OpenAI GPT-4
2. OpenAI kh√¥ng ph·∫£n h·ªìi (rate limit)
3. System log: "Provider openai is not healthy, attempting fallback"
4. Fallback sang OpenRouter v·ªõi model: openai/gpt-4o-mini
5. Response th√†nh c√¥ng t·ª´ OpenRouter
```

### Scenario 2: T·∫•t C·∫£ Fail Except Local
```
1. Request ƒë·∫øn
2. OpenAI: ‚ùå No API key
3. Claude: ‚ùå No API key  
4. OpenRouter: ‚ùå Rate limited
5. System log: "Falling back to OSS Local model"
6. S·ª≠ d·ª•ng Ollama local: llama3:8b
7. Response t·ª´ local model
```

## üéØ Best Practices

### 1. **Cho Production**
- C·∫•u h√¨nh √≠t nh·∫•t 2 providers (v√≠ d·ª•: OpenAI + OpenRouter)
- B·∫≠t cost tracking
- Set log level = "info"

### 2. **Cho Development**
- D√πng OpenRouter v·ªõi free models
- B·∫≠t OSS Local n·∫øu mu·ªën offline development
- Set log level = "debug" ƒë·ªÉ xem chi ti·∫øt

### 3. **Cho Cost Optimization**
- D√πng OpenRouter free models l√†m primary
- OpenAI/Claude ch·ªâ cho critical tasks
- B·∫≠t auto-escalate ƒë·ªÉ ch·ªâ d√πng models ƒë·∫Øt khi c·∫ßn

## üõ†Ô∏è T√πy Ch·ªânh Models

### OpenRouter Free Models (Recommended)

```bash
# T·ªët cho code
OPENROUTER_FALLBACK_MODELS=qwen/qwen-2.5-coder-32b-instruct,deepseek/deepseek-coder:free

# T·ªët cho general chat
OPENROUTER_FALLBACK_MODELS=meta-llama/llama-3.1-8b-instruct:free,mistralai/mistral-7b-instruct:free

# Balanced
OPENROUTER_FALLBACK_MODELS=x-ai/grok-beta,qwen/qwen-2.5-coder-32b-instruct,meta-llama/llama-3.1-8b-instruct:free
```

### OpenRouter Paid Models (Better Quality)

```bash
# Thay th·∫ø OpenAI v·ªõi paid model t·ªët h∆°n
OPENROUTER_REPLACE_OPENAI=openai/gpt-4o

# Thay th·∫ø Claude v·ªõi paid model
OPENROUTER_REPLACE_CLAUDE=anthropic/claude-3.5-sonnet
```

## üìù Ki·ªÉm Tra Health Status

Server expose endpoint ƒë·ªÉ check provider status:

```bash
# Health check
curl http://localhost:3000/health

# Response:
{
  "status": "ok",
  "providers": {
    "openai": true,
    "anthropic": false,
    "openrouter": true,
    "oss-local": false
  }
}
```

## ‚ö†Ô∏è Troubleshooting

### Provider Kh√¥ng ƒê∆∞·ª£c Detect
- Ki·ªÉm tra API key ƒë√∫ng format
- Verify network connection
- Check log xem c√≥ error g√¨

### Fallback Kh√¥ng Ho·∫°t ƒê·ªông
- ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 1 backup provider configured
- Check OPENROUTER_API_KEY c√≥ h·ª£p l·ªá
- Xem log ƒë·ªÉ bi·∫øt provider n√†o ƒëang fail

### OSS Local Kh√¥ng Connect
- ƒê·∫£m b·∫£o Ollama ƒë√£ ch·∫°y: `ollama serve`
- Pull model tr∆∞·ªõc: `ollama pull llama3:8b`
- Check endpoint: `curl http://localhost:11434/api/tags`

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, check:
1. Log file: `logs/ai-mcp-gateway.log`
2. Console output khi start server
3. Provider health status trong log

---

**L∆∞u √Ω:** Health check cache k·∫øt qu·∫£ trong 1 ph√∫t. Provider b·ªã ƒë√°nh d·∫•u unhealthy s·∫Ω ƒë∆∞·ª£c retry sau 60 gi√¢y.
