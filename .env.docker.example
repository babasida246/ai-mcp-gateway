# AI MCP Gateway - Environment Variables
# Copy this file to .env.docker and customize with your values

# ============================================
# LLM Provider API Keys
# ============================================
# REQUIRED: At least one provider API key
# You have OpenRouter - this enables L1 layer
OPENROUTER_API_KEY=sk-or-openrouter-key-here

# Optional: Add these to enable more layers
# L1, L2: Uncomment to use OpenAI directly (or use via OpenRouter)
# OPENAI_API_KEY=sk-your-openai-key-here

# L2, L3: Uncomment to use Anthropic Claude models
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# NOTE: With only OpenRouter key, you can still use all models via OpenRouter proxy
# OpenRouter supports: OpenAI models, Anthropic models, Google models, etc.
# Just use the OpenRouter model names in your requests

# ============================================
# OpenRouter Configuration  
# ============================================
# Free models for L0 layer (fallback)
OPENROUTER_FALLBACK_MODELS=meta-llama/llama-3.3-70b-instruct:free,x-ai/grok-4.1-fast:free

# Use OpenRouter to proxy OpenAI/Claude models (if you don't have direct API keys)
OPENROUTER_REPLACE_OPENAI=openai/gpt-4o-mini
OPENROUTER_REPLACE_CLAUDE=anthropic/claude-3.5-sonnet

# Layer override - customize models per layer using OpenRouter
# Format: LAYER_L0_MODELS, LAYER_L1_MODELS, etc.
LAYER_L0_MODELS=meta-llama/llama-3.3-70b-instruct:free,x-ai/grok-4.1-fast:free
LAYER_L1_MODELS=google/gemini-flash-1.5,openai/gpt-4o-mini
LAYER_L2_MODELS=anthropic/claude-3-haiku,openai/gpt-4o
LAYER_L3_MODELS=anthropic/claude-3.5-sonnet,openai/o1-preview

# ============================================
# Task-Specific Model Configuration
# ============================================
# Models for different tasks (comma-separated)
# Chat - General conversation
CHAT_MODELS=meta-llama/llama-3.3-70b-instruct:free,google/gemini-flash-1.5

# Code - Code generation and analysis (prefer models with 'code' or 'coder' in name)
CODE_MODELS=qwen/qwen-2.5-coder-32b-instruct:free,deepseek/deepseek-coder-33b-instruct:free

# Analyze - Code review and analysis
ANALYZE_MODELS=x-ai/grok-4.1-fast:free,anthropic/claude-3-haiku

# Create Project - Full project scaffolding
CREATE_PROJECT_MODELS=qwen/qwen-2.5-coder-32b-instruct:free,openai/gpt-4o-mini

# ============================================
# OSS/Local Model (Ollama)
# ============================================
# Set to true if using Ollama (requires: docker-compose --profile with-ollama up)
OSS_MODEL_ENABLED=false
OSS_MODEL_ENDPOINT=http://ollama:11434
OSS_MODEL_NAME=llama3:8b

# ============================================
# Database (PostgreSQL)
# ============================================
POSTGRES_DB=ai_mcp_gateway
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_postgres_password_here

# ============================================
# Redis Cache
# ============================================
# Leave empty for no password (development only)
REDIS_PASSWORD=

# ============================================
# Application Configuration
# ============================================
LOG_LEVEL=info
DEFAULT_LAYER=L0
ENABLE_CROSS_CHECK=true
ENABLE_AUTO_ESCALATE=false
MAX_ESCALATION_LAYER=L0
ENABLE_COST_TRACKING=true
COST_ALERT_THRESHOLD=1.00

# ============================================
# Task-Specific Model Selection
# ============================================
# Code generation tasks - prefer models with "code" or "coder" in name
CODE_MODELS=qwen/qwen-2.5-coder-32b-instruct:free,deepseek/deepseek-coder-33b-instruct:free

# General chat/conversational tasks
CHAT_MODELS=meta-llama/llama-3.3-70b-instruct:free,x-ai/grok-4.1-fast:free

# Code analysis tasks - prefer models with good reasoning
ANALYZE_MODELS=qwen/qwq-32b-preview:free,x-ai/grok-4.1-fast:free

# Project creation tasks - prefer models with both code and reasoning capabilities
CREATE_PROJECT_MODELS=qwen/qwen-2.5-coder-32b-instruct:free,deepseek/deepseek-coder-33b-instruct:free

